{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "-Kee-DAl2viO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Hands on Machine learning *\n",
        "# **unsupervised machine learning project k means cluster project**"
      ],
      "metadata": {
        "id": "u7X_pJJtUjyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -**\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "# This project presents a comprehensive approach to analyzing a dataset through a combination of unsupervised learning (K-Means clustering), text preprocessing, and machine learning classification models. The primary objective was to explore the dataset, extract meaningful features from textual data, and apply clustering to uncover natural groupings within the data. Subsequently, supervised machine learning models—Decision Trees and Random Forests—were implemented to predict the clusters and evaluate the model performance."
      ],
      "metadata": {
        "id": "ul_4MrVOcjNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yCyjiBIeFZLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With the vast and ever-growing catalog of content on platforms like Netflix, users often face difficulties in discovering relevant movies and TV shows. The challenge lies in efficiently organizing and categorizing content based on its descriptions and metadata. This project aims to apply K-Means clustering on a Netflix dataset, combining TF-IDF vectorization to process text data, to automatically group similar items together. The goal is to uncover hidden patterns within the dataset, allowing for improved content organization and more accurate content recommendations.**"
      ],
      "metadata": {
        "id": "i9pq4u99fUbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import statsmodels.api as sm\n",
        "import statistics as stat\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BlpwtazPcqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YHgR9rc-PdDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df=pd.read_csv('/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING (3).csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "0pHoQvE_7Nlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicates\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Handle missing values (for simplicity, fill with 'Unknown' for categorical data)\n",
        "df.fillna({'country': 'Unknown', 'rating': 'Unknown'}, inplace=True)\n",
        "\n",
        "# Check outliers in the 'release_year' column\n",
        "plt.boxplot(df['release_year'])\n",
        "plt.title('Boxplot of Release Year')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O6oEa31GNc3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jKG7dBGnaa38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "3mI6GYo5aZBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKY1WzH0dUIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#Choose the number of clusters\n",
        "df['text'] = df['description'] + \" \" + df['listed_in']  # Combine both columns\n",
        "\n",
        "#Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "#Fit and transform the combined text to create the TF-IDF matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "num_clusters = 5\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(tfidf_matrix)\n",
        "\n",
        "#Assign clusters back to the DataFrame\n",
        "df['cluster'] = kmeans.labels_\n",
        "df"
      ],
      "metadata": {
        "id": "MmGUkSgPlxh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='type', palette='Set2')\n",
        "plt.title('Distribution of Movies and TV Shows')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The countplot was chosen because it provides a straightforward and effective way to visualize the distribution of categorical data."
      ],
      "metadata": {
        "id": "UyRsN4eYkcCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distribution of Content: The countplot shows how the dataset is distributed between movies and TV shows."
      ],
      "metadata": {
        "id": "bpHakt0okgRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The countplot offers valuable insights into the content distribution between movies and TV shows, which can guide decisions in content strategy, recommendations, and marketing."
      ],
      "metadata": {
        "id": "aCxkuVs_kxr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "country_counts = df['country'].value_counts().head(10)\n",
        "sns.barplot(x=country_counts.index, y=country_counts.values, palette='coolwarm')\n",
        "plt.title('Top 10 Countries by Number of Titles')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Number of Titles')\n",
        "plt.xlabel('Country')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# here i choose bar chart because bar provide clear comparison between diffrent element"
      ],
      "metadata": {
        "id": "QW8wu-iTlpRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The bar chart shows the top 10 countries with the most titles in the dataset. This insight reveals which countries have the largest content production or availability on Netflix."
      ],
      "metadata": {
        "id": "21SZxgkJlfYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The insights from this chart can help Netflix refine its content strategy, especially in regions with high content volume"
      ],
      "metadata": {
        "id": "KhscYZHlliMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='rating', palette='Pastel1', order=df['rating'].value_counts().index)\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I chose the countplot for visualizing the distribution of ratings because it provides a clear view of the frequency of each rating category (e.g., PG, PG-13, R, etc.) in the datase"
      ],
      "metadata": {
        "id": "2Fl7v7btmOrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From this chart, we can derive insights into the distribution of ratings across the dataset."
      ],
      "metadata": {
        "id": "Cv7K429OmWvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZH2DXuukmhuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The insights can drive positive business impact by helping Netflix tailor its content to specific demographic groups"
      ],
      "metadata": {
        "id": "xtC6YwogmjQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(df['release_year'], bins=30, kde=True, color='purple')\n",
        "plt.title('Distribution of Release Years')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I chose the histogram with a KDE (Kernel Density Estimate) overlay to visualize the distribution of release years because it effectively shows the frequency of content released in different years"
      ],
      "metadata": {
        "id": "QoJqp11LnIC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The histogram reveals insights into the release patterns of content over the years"
      ],
      "metadata": {
        "id": "-qwcONmEnLzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insights from this chart can help Netflix understand historical content trends and plan its future release strategy"
      ],
      "metadata": {
        "id": "cCFU29XKnTAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df, x='type', y='release_year', palette='Set3')\n",
        "plt.title('Release Year by Content Type')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Release Year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I chose a boxplot because it is effective for visualizing the distribution of numerical data (in this case, release_year) across different categories (here, type, which could be \"movie\" or \"TV show\")"
      ],
      "metadata": {
        "id": "pnBxiI_PnvBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insight is founding from this chart is\n",
        "# Release Year Trends,\n",
        "# Spread and Outliers,\n",
        "# Comparison between Movies and TV Shows"
      ],
      "metadata": {
        "id": "ATuoJUr2n778"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m_SXt2EAn01A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If Netflix identifies a higher concentration of newer TV shows compared to movies (or vice versa), it can refine its content strategy. For example, if there is an underrepresentation of movies from recent years, Netflix might choose to acquire or produce more recent movies to attract viewers who prefer new content."
      ],
      "metadata": {
        "id": "YrK36dsloU99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df, x='rating', hue='type', palette='colorblind')\n",
        "plt.title('Count of Movies and TV Shows by Rating')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Rating')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I chose a countplot with hue to compare the distribution of movies and TV shows across different ratings."
      ],
      "metadata": {
        "id": "b8h7NdUnpe4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The countplot shows the distribution of content ratings (e.g., PG, PG-13, R, etc.) for both movies and TV shows. This allows us to identify which ratings are more common for each content type."
      ],
      "metadata": {
        "id": "CeAvrVdCphdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the distribution of ratings across content types allows Netflix to tailor its content acquisition and production strategies"
      ],
      "metadata": {
        "id": "Drc1fvAyp0Ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "genres =df['listed_in'].str.split(', ', expand=True).stack().value_counts().head(10)\n",
        "sns.barplot(x=genres.index, y=genres.values, palette='viridis')\n",
        "plt.title('Top 10 Genres in Movies')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Genre')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I chose a barplot to visualize the top 10 most common genres in the dataset because it is effective at displaying the frequency of each genre."
      ],
      "metadata": {
        "id": "XJgTx0J4qJ5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The barplot shows the top 10 most frequent genres across the entire dataset, indicating the genres that are most commonly represented in Netflix's movie catalog. For instance, genres like \"Drama,\" \"Comedy,\" or \"Action\" might be among the top genres."
      ],
      "metadata": {
        "id": "3IvQXYVRqNWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# By understanding which genres are most popular, Netflix can ensure that it continues to invest in or acquire more content within those genres to meet user demand"
      ],
      "metadata": {
        "id": "mu9ytDIhqWa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df, x='release_year', y='rating', palette='Set2')\n",
        "plt.title('Rating vs. Release Year')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Rating')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I chose a boxplot for this visualization because it allows for an effective comparison of the distribution of ratings across different release years"
      ],
      "metadata": {
        "id": "NF4kHNVWr0ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rating Distribution Over Time: The chart shows how the distribution of ratings has changed over the years. For example, we may observe that older content tends to have a higher rating (e.g., PG, PG-13), while newer content might show a broader range of ratings, including more adult content (e.g., R or TV-MA)."
      ],
      "metadata": {
        "id": "Wf1lHBuzuron"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# By understanding how ratings have shifted over time, Netflix can fine-tune its content strategy. For example, if the chart shows a growing trend toward mature content (R or TV-MA), Netflix could continue investing in adult-oriented shows and movies to attract a mature audience."
      ],
      "metadata": {
        "id": "InTZod2Uu8YD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "df['description_length'] = df['description'].str.len()\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(df['description_length'], bins=30, kde=True, color='orange')\n",
        "plt.title('Distribution of Description Lengths')\n",
        "plt.xlabel('Length of Description')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I chose a histogram with a KDE (Kernel Density Estimate) overlay to visualize the distribution of description lengths because it allows us to see the spread and density of the data in a clear and intuitive way."
      ],
      "metadata": {
        "id": "px23pPj0wjGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# the histogram shows a peak at lower description lengths, it suggests that most content on Netflix has relatively short descriptions"
      ],
      "metadata": {
        "id": "Ykc9jllswl-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# By understanding the length of descriptions, Netflix can optimize how content is presented to users. If most descriptions are short and to the point, Netflix can ensure that this aligns with user preferences for quick browsing"
      ],
      "metadata": {
        "id": "CkzT-y7Xw0ma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "df['year_counts'] =df['release_year'].value_counts()\n",
        "sns.lineplot(data=df['release_year'].value_counts().sort_index())\n",
        "plt.title('Content Released Over the Years')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Titles')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# I chose a line plot to visualize the trend of content releases over the years because a line plot effectively illustrates how the number of titles released by Netflix has changed over time"
      ],
      "metadata": {
        "id": "YJJ2R7IExcUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# the line shows a steady upward trend, it indicates that Netflix has been increasing the volume of content released over the years"
      ],
      "metadata": {
        "id": "MvJYVz6Rx79i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# By analyzing content release trends, Netflix can better plan for future content investments. If there is a noticeable spike in releases during certain years, Netflix could use this data to predict when it might need additional resources or to push even further into content creation."
      ],
      "metadata": {
        "id": "1b6TtjjzyDsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample DataFrame for demonstration (replace this with your actual DataFrame)\n",
        "# df = pd.DataFrame(...)  # Your actual DataFrame containing 'country' and 'type'\n",
        "\n",
        "# Check if 'country' and 'type' columns exist in the DataFrame\n",
        "if 'country' in df.columns and 'type' in df.columns:\n",
        "    # Group by country and type, then count the titles\n",
        "    country_type_counts = df.groupby(['country', 'type']).size().unstack().fillna(0)\n",
        "\n",
        "    # Calculate the total count of titles per country\n",
        "    country_totals = country_type_counts.sum(axis=1)\n",
        "\n",
        "    # Get the top 5 countries\n",
        "    top_countries = country_totals.nlargest(5).index\n",
        "\n",
        "    # Filter the original counts to include only the top 5 countries\n",
        "    top_country_type_counts = country_type_counts.loc[top_countries]\n",
        "\n",
        "    # Create the stacked bar chart\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    top_country_type_counts.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
        "    plt.title('Count of Titles by Top 5 Countries and Type')\n",
        "    plt.xlabel('Country')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.legend(title='Type')\n",
        "    plt.tight_layout()  # Adjust layout to prevent clipping\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"The DataFrame must contain 'country' and 'type' columns.\")\n"
      ],
      "metadata": {
        "id": "F0UdDG7CTAm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I chose a stacked bar chart to visualize the distribution of titles by country and type (e.g., movie or TV show) for the top 5 countries because it allows us to see both the total number of titles per country as well as how they are broken down by type."
      ],
      "metadata": {
        "id": "0PQQzICv0BGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We can clearly see the number of movies vs. TV shows produced in each of the top 5 countries. For instance, some countries might have a higher count of movies, while others may be more focused on TV shows."
      ],
      "metadata": {
        "id": "FL5eecDM0IYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# By understanding the distribution of content types (movies vs. TV shows) across countries, Netflix can tailor its content acquisition strategy. For instance, if a country has a higher production of movies, Netflix may focus on promoting or acquiring more movies from that country. Conversely, if another country is rich in TV shows, Netflix could prioritize those to better cater to regional audience preferences.\n"
      ],
      "metadata": {
        "id": "TTA0qO8n03-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample DataFrame for demonstration (replace this with your actual DataFrame)\n",
        "# df = pd.DataFrame(...)  # Your actual DataFrame containing numeric features\n",
        "\n",
        "# Select only numeric features for correlation\n",
        "numeric_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Heatmap of Correlation Between Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zeBFyJuTTpGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The heatmap is an ideal chart for visualizing correlations between numerical variables in a dataset. Correlation analysis helps us understand the relationships between different features, such as whether they move together (positive correlation) or in opposite directions (negative correlation)."
      ],
      "metadata": {
        "id": "G-6JQakf2JdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features that have a strong positive correlation (values near 1) indicate that as one feature increases, the other feature tends to increase as well. For example, if two financial variables (e.g., \"Revenue\" and \"Profit\") are highly positively correlated, increasing revenue tends to increase profit."
      ],
      "metadata": {
        "id": "jzgw7tfK2MOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding which features are strongly correlated can inform better business decisions. For example, if \"Marketing Spend\" and \"Sales Revenue\" are highly correlated, a business might decide to allocate more resources to marketing efforts to drive revenue growth."
      ],
      "metadata": {
        "id": "RJ0jA9AK2UrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df, x='cluster', palette='Set2')\n",
        "plt.title('Count of Movies and TV Shows in Each Cluster (KMeans)')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The countplot is an effective choice for visualizing the distribution of categorical data, such as the number of movies and TV shows in each cluster after performing KMeans clustering."
      ],
      "metadata": {
        "id": "iI-lYfNu3Ep9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# : The chart shows the number of movies or TV shows in each cluster, which helps you understand how the KMeans algorithm has distributed the data points across different clusters."
      ],
      "metadata": {
        "id": "vi49rS6Y3iao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding how many movies and TV shows belong to each cluster can help tailor marketing strategies or content recommendations."
      ],
      "metadata": {
        "id": "g5IfBYMJ3nCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6FlUIVYA3m1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df, x='cluster',palette='viridis')\n",
        "plt.title('Count of Movies and TV Shows in Each Cluster (DBSCAN)')\n",
        "plt.xlabel('DBSCAN Cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The countplot is an appropriate choice for visualizing the distribution of categorical data, such as the number of movies and TV shows assigned to each cluster after performing DBSCAN (Density-Based Spatial Clustering of Applications with Noise)"
      ],
      "metadata": {
        "id": "gFgaS67I4WIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The chart shows how the movies and TV shows are distributed across the clusters identified by DBSCAN. This helps you understand whether the algorithm has identified well-defined groups of similar data points, and how many items belong to each group."
      ],
      "metadata": {
        "id": "hW3b_Otx5EcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "for i in range(num_clusters):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    cluster_titles = df[df['cluster'] == i]['title'].head(10)\n",
        "    sns.barplot(x=cluster_titles.index, y=cluster_titles.values, palette='pastel')\n",
        "    plt.title(f'Top Titles in Cluster {i}')\n",
        "    plt.xlabel('Title')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The code provided generates a barplot for each cluster, displaying the top titles (e.g., movies or TV shows) in each cluster"
      ],
      "metadata": {
        "id": "cYLwy79T5Wzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Each plot will show the most frequent or notable titles within a specific cluster. This helps you understand which types of content are grouped together, based on the features used in the clustering process."
      ],
      "metadata": {
        "id": "yRD0Mp7Q5ewi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Null Hypothesis (H₀): There is no significant difference in the average release year between movies and TV shows.\n",
        "# Alternative Hypothesis (H₁): There is a significant difference in the average release year between movies and TV shows."
      ],
      "metadata": {
        "id": "BUrFo14XAs8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value# Hypothesis 1: Average release year of movies vs. TV shows\n",
        "\n",
        "# Separate the data\n",
        "movies_years = df[df['type'] == 'Movie']['release_year']\n",
        "tv_shows_years = df[df['type'] == 'TV Show']['release_year']\n",
        "\n",
        "# Perform independent t-test\n",
        "t_stat, p_value = stats.ttest_ind(movies_years.dropna(), tv_shows_years.dropna())\n",
        "alpha = 0.05  # Significance level\n",
        "\n",
        "# Print results for Hypothesis 1\n",
        "print(\"Hypothesis 1: Average Release Year of Movies vs TV Shows\")\n",
        "print(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference in the average release year.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant difference in the average release year.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The statistical test performed is an Independent Two-Sample t-test (also known as Student's t-test)."
      ],
      "metadata": {
        "id": "BOtZh2-i6LYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The goal is to compare the average release year between two independent groups (movies and TV shows). We want to determine if there is a significant difference in the means of the release years of the two types of content."
      ],
      "metadata": {
        "id": "GL1op-MD6Y5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Null Hypothesis (H₀): The distribution of ratings is the same for movies and TV shows (i.e., the ratings are independent of the type of content).\n",
        "\n",
        "# Alternative Hypothesis (H₁): The distribution of ratings is different for movies and TV shows (i.e., the ratings are dependent on the type of content)."
      ],
      "metadata": {
        "id": "ETkztJzOABRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rating_crosstab = pd.crosstab(df['type'], df['rating'])\n",
        "chi2_stat, p_value_chi2, dof, expected = stats.chi2_contingency(rating_crosstab)\n",
        "\n",
        "# Print results for Hypothesis 2\n",
        "print(\"\\nHypothesis 2: Distribution of Ratings for Movies and TV Shows\")\n",
        "print(f\"Chi-Square Statistic: {chi2_stat}, P-Value: {p_value_chi2}\")\n",
        "if p_value_chi2 < alpha:\n",
        "    print(\"Reject the null hypothesis: The distribution of ratings is not the same for movies and TV shows.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The distribution of ratings is the same for movies and TV shows.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The statistical test used to obtain the p-value is the Chi-Square Test of Independence (also known as Chi-Square Test for Association)"
      ],
      "metadata": {
        "id": "_nfUZQuCAL3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# The objective is to determine if there is a relationship or difference in the distribution of ratings between movies and TV shows."
      ],
      "metadata": {
        "id": "5MUBFywzAdK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "# There is no significant difference in the average description length between the selected genres (Drama, Comedy, Action). Mathematically\n",
        "Alternative Hypothesis (H₁):\n",
        "# There is a significant difference in the average description length between the selected genres (Drama, Comedy, Action). Mathematically"
      ],
      "metadata": {
        "id": "EhKwmt_fCZgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Calculate description length\n",
        "df['description_length'] = df['description'].str.len()\n",
        "\n",
        "# Separate genres into a long format\n",
        "genres = df['listed_in'].str.split(', ', expand=True).stack()\n",
        "genres = genres.reset_index(level=1, drop=True)  # Reset the index to align with the original DataFrame\n",
        "genres.name = 'genre'  # Name the Series\n",
        "\n",
        "# Combine with the original DataFrame\n",
        "long_format = df.join(genres).drop(columns=['listed_in'])\n",
        "\n",
        "# Now we can perform ANOVA on specific genres\n",
        "# Ensure to select genres present in the dataset\n",
        "selected_genres = ['Drama', 'Comedy', 'Action']\n",
        "data_for_anova = [long_format[long_format['genre'] == genre]['description_length'].dropna() for genre in selected_genres]\n",
        "\n",
        "# Perform ANOVA test\n",
        "anova_result = stats.f_oneway(*data_for_anova)\n",
        "\n",
        "# Print results for Hypothesis 3\n",
        "print(\"\\nHypothesis 3: Average Description Length Differs Between Genres\")\n",
        "print(f\"F-Statistic: {anova_result.statistic}, P-Value: {anova_result.pvalue}\")\n",
        "if anova_result.pvalue < 0.05:  # Significance level\n",
        "    print(\"Reject the null hypothesis: The average description length differs between genres.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant difference in the average description length between genres.\")\n"
      ],
      "metadata": {
        "id": "nO0vkHWJWhXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The appropriate statistical test in this case is the Analysis of Variance (ANOVA) test"
      ],
      "metadata": {
        "id": "2GUgL-SnBtmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The goal is to determine if there is a significant difference in the average description length across multiple genres (Drama, Comedy, Action).\n",
        "\n"
      ],
      "metadata": {
        "id": "2TV_7UuqCDf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df=pd.read_csv('/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING (3).csv')"
      ],
      "metadata": {
        "id": "BswVDTauGZda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For missing values, categorical columns like director, cast, and country were imputed with the placeholder 'Unknown', while the numerical column release_year was imputed with the mean value."
      ],
      "metadata": {
        "id": "sRjBEOAVnjkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "1mUTKbsZsrI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NWUr-zK0srDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the initial number of missing values\n",
        "print(\"Missing values before handling:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 1. Handling Missing Values\n",
        "# Fill missing values for categorical columns\n",
        "df.fillna({'country': 'Unknown','director':'unknown', 'rating':'Unknown','cast':'unknown'}, inplace=True)\n",
        "\n",
        "# Fill missing values for numerical columns (example: 'release_year')\n",
        "df['release_year'].fillna(df['release_year'].mean(), inplace=True)\n",
        "\n",
        "# Check for missing values after handling\n",
        "print(\"\\nMissing values after handling:\")\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "MZSRcU71o7P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Y8pmy36trwZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualize outliers in 'release_year'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=df['release_year'])\n",
        "plt.title('Boxplot of Release Year')\n",
        "plt.show()\n",
        "\n",
        "# Define a function to handle outliers using IQR method\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "# Remove outliers from the 'release_year' column\n",
        "netflix_data = remove_outliers(df, 'release_year')\n",
        "\n",
        "# Check the shape of the DataFrame after outlier removal\n",
        "print(\"\\nShape of DataFrame after outlier removal:\", netflix_data.shape)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outliers in the release_year were handled using the Interquartile Range (IQR) method, removing data points outside the range defined by 1.5 times the IQR. This approach ensures consistency while maintaining the integrity of the data."
      ],
      "metadata": {
        "id": "meBwmdkxnvfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical column\n",
        "# Convert categorical columns to categorical data type\n",
        "df['country'] = df['country'].astype('category')\n",
        "df['rating'] = df['rating'].astype('category')\n",
        "\n",
        "# One-hot encoding for categorical variables\n",
        "netflix_data_encoded = pd.get_dummies(netflix_data, columns=['country', 'rating'], drop_first=True)\n",
        "\n",
        "# Display the shape of the DataFrame after encoding\n",
        "print(\"\\nShape of DataFrame after one-hot encoding:\", netflix_data_encoded.shape)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In the provided code, One-hot encoding was applied to the country and rating categorical columns, one-hot encoding is used because it transforms categorical variables into a format that can be provided to machine learning algorithms"
      ],
      "metadata": {
        "id": "wehqvoYXoalY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the 'description' column\n",
        "df['description'] = df['description'].apply(preprocess_text)\n",
        "\n",
        "# Combine title and description for better context\n",
        "df['text'] = df['title'] + ' ' + df['description']\n",
        "\n",
        "# TF-IDF Vectorization for text features\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Make sure that column names are treated as strings before fitting TF-IDF\n",
        "df['text'] = df['text'].astype(str)\n",
        "\n",
        "# Apply the TfidfVectorizer\n",
        "tfidf_matrix = tfidf.fit_transform(df['text'])\n",
        "\n",
        "# Display the shape of the TF-IDF matrix\n",
        "print(\"\\nShape of TF-IDF matrix:\", tfidf_matrix.shape)\n"
      ],
      "metadata": {
        "id": "MDxo1rMgpi4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I use lower cassing , removing numbers and punctutatation technique becauseThese normalization steps are standard in text preprocessing. Lowercasing ensures that variations in case do not affect text interpretation, while removing numbers and punctuation keeps the focus on the meaningful content (words)."
      ],
      "metadata": {
        "id": "P2Ds5xPgqQ8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The TF-IDF (Term Frequency-Inverse Document Frequency) method was used to vectorize the text"
      ],
      "metadata": {
        "id": "KYYTqeWaq2Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Create new features if needed (example: length of title and description)\n",
        "df['title_length'] = df['title'].str.len()\n",
        "df['description_length'] = df['description'].str.len()\n",
        "\n",
        "# Selecting relevant features for modeling\n",
        "features = df[['release_year', 'title_length', 'description_length']]\n",
        "features = pd.concat([features, pd.DataFrame(tfidf_matrix.toarray())], axis=1)\n",
        "\n",
        "# Display the shape of the features DataFrame\n",
        "print(\"\\nShape of features DataFrame:\", features.shape)\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yes, the data does need transformation. In this case, a log transformation was applied to the release_year feature using np.log1p()This transformation is useful when the data is skewed or has a large range, as it helps normalize the distribution and reduce the impact of extreme values."
      ],
      "metadata": {
        "id": "gfjO_5M_sSt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Example of transforming features (if needed)\n",
        "# Here we can log-transform the release year to normalize the data\n",
        "features['release_year'] = np.log1p(features['release_year'])\n",
        "\n",
        "# Display the transformed feature\n",
        "print(\"\\nTransformed release_year (log):\")\n",
        "print(features['release_year'].head())\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features = pd.DataFrame(df)\n",
        "\n",
        "# Select only numeric features\n",
        "numeric_features = features.select_dtypes(include=[np.number])\n",
        "\n",
        "# Check if there are any non-numeric types and print the columns\n",
        "non_numeric_columns = features.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "if non_numeric_columns:\n",
        "    print(\"Non-numeric columns found:\", non_numeric_columns)\n",
        "\n",
        "# Scale the numeric features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(numeric_features)\n",
        "\n",
        "# Convert the scaled features back to a DataFrame\n",
        "scaled_features_df = pd.DataFrame(scaled_features, columns=numeric_features.columns)\n",
        "\n",
        "# Display the shape of the scaled features\n",
        "print(\"\\nShape of scaled features:\", scaled_features_df.shape)\n",
        "\n",
        "# Display the first few rows of the scaled features\n",
        "print(scaled_features_df.head())\n"
      ],
      "metadata": {
        "id": "kvEk73SMf9pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The **StandardScaler** method was used to scale the data. It standardizes the numeric features by removing the mean and scaling to unit variance, ensuring that all features contribute equally to the model, especially when the data has different units or scales."
      ],
      "metadata": {
        "id": "Jfl88ObssukY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yes, dimensionality reduction is needed to simplify the dataset and reduce the risk of overfitting. By using **PCA**, we retain 95% of the variance while reducing the number of features, which helps improve model performance and computational efficiency."
      ],
      "metadata": {
        "id": "478rDOVJs9Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA to reduce dimensionality\n",
        "pca = PCA(n_components=0.95)  # Keep 95% variance\n",
        "reduced_features = pca.fit_transform(scaled_features)\n",
        "\n",
        "# Display the shape of the reduced features\n",
        "print(\"\\nShape of reduced features after PCA:\", reduced_features.shape)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test = train_test_split(reduced_features, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the shapes of the training and testing sets\n",
        "print(\"\\nShape of training set:\", X_train.shape)\n",
        "print(\"Shape of testing set:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The data was split using an 80-20 split, where 80% of the data is used for training and 20% is used for testing"
      ],
      "metadata": {
        "id": "DTJGOfPSuRL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whether the dataset is imbalanced depends on the distribution of the target variable."
      ],
      "metadata": {
        "id": "vuIgCkuzulHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Check for class imbalance in the target variable (example: if a target column exists)\n",
        "# Assuming we have a 'target' column for illustration\n",
        "# print(netflix_data['target'].value_counts())\n",
        "\n",
        "# If imbalanced, use techniques like SMOTE or Random Under-Sampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Assuming we have a target variable for the sake of demonstration\n",
        "# X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)\n",
        "\n",
        "# Display the new class distribution after resampling\n",
        "# print(pd.Series(y_train_resampled).value_counts())\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If the dataset is imbalanced, SMOTE (Synthetic Minority Over-sampling Technique) can be used to handle the imbalance. SMOTE generates synthetic samples for the minority class to balance the class distribution in the training dataset."
      ],
      "metadata": {
        "id": "xWbyl7MkuzGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vkNpe0rvBWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Sample DataFrame for demonstration (replace this with your actual DataFrame)\n",
        "# df = pd.DataFrame(...)  # Your actual DataFrame containing data\n",
        "\n",
        "# Select only numeric features\n",
        "numeric_features = df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Scale the numeric features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(numeric_features)\n",
        "\n",
        "# Convert the scaled features back to a DataFrame\n",
        "scaled_features_df = pd.DataFrame(scaled_features, columns=numeric_features.columns)\n",
        "\n",
        "# --- Initial Clustering (Before Hyperparameter Tuning) ---\n",
        "# Set the number of clusters and fit KMeans with default parameters\n",
        "n_clusters = min(5, max(2, len(df) - 1))  # Adjust clusters based on data size\n",
        "kmeans_initial = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans_initial.fit(scaled_features_df)\n",
        "\n",
        "# Assign initial cluster labels to the original DataFrame\n",
        "df['kmeans_cluster_initial'] = kmeans_initial.labels_\n",
        "\n",
        "# Evaluate clustering performance using silhouette score for initial clustering\n",
        "silhouette_initial = silhouette_score(scaled_features_df, kmeans_initial.labels_)\n",
        "print(f\"Initial Silhouette Score for KMeans: {silhouette_initial:.2f}\")\n",
        "\n",
        "# --- Hyperparameter Tuning (Using GridSearchCV) ---\n",
        "def silhouette_scorer(estimator, X):\n",
        "    labels = estimator.labels_\n",
        "    return silhouette_score(X, labels)\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_clusters': [2, 3, 4, 5, 6, 7, 8, 9, 10],  # Number of clusters to try\n",
        "    'init': ['k-means++', 'random'],  # Initialization methods\n",
        "    'n_init': [10, 20],  # Number of times to run the algorithm with different initializations\n",
        "    'max_iter': [100, 300, 500],  # Maximum number of iterations\n",
        "    'tol': [1e-4, 1e-3]  # Tolerance for convergence\n",
        "}\n",
        "\n",
        "# Initialize KMeans (this will be used by GridSearchCV)\n",
        "kmeans = KMeans(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV with the custom silhouette scorer\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=kmeans,\n",
        "    param_grid=param_grid,\n",
        "    scoring=silhouette_scorer,\n",
        "    cv=3,  # Cross-validation splitting strategy\n",
        "    verbose=1,\n",
        "    n_jobs=-1  # Use all available cores for parallel processing\n",
        ")\n",
        "\n",
        "# Perform the grid search over the parameter grid\n",
        "grid_search.fit(scaled_features_df)\n",
        "\n",
        "# Get the best hyperparameters and the best silhouette score from GridSearchCV\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Silhouette Score from GridSearchCV:\", grid_search.best_score_)\n",
        "\n",
        "# Get the best KMeans model after tuning\n",
        "best_kmeans = grid_search.best_estimator_\n",
        "\n",
        "# Assign the best cluster labels to the original DataFrame\n",
        "df['kmeans_cluster_best'] = best_kmeans.labels_\n",
        "\n",
        "# Evaluate the clustering performance again using silhouette score\n",
        "silhouette_best = silhouette_score(scaled_features_df, best_kmeans.labels_)\n",
        "print(f\"Final Silhouette Score for KMeans with tuned parameters: {silhouette_best:.2f}\")\n",
        "\n",
        "# Display the initial and final cluster distribution\n",
        "print(\"\\nCluster distribution (initial):\")\n",
        "print(df['kmeans_cluster_initial'].value_counts())\n",
        "\n",
        "print(\"\\nCluster distribution (tuned):\")\n",
        "print(df['kmeans_cluster_best'].value_counts())\n",
        "\n",
        "# Plot the improvement (if needed)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the silhouette scores for comparison\n",
        "scores = [silhouette_initial, silhouette_best]\n",
        "labels = ['Initial Clustering', 'Tuned Clustering']\n",
        "\n",
        "plt.bar(labels, scores, color=['blue', 'green'])\n",
        "plt.xlabel('Clustering Method')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Improvement in Clustering (Silhouette Score Comparison)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qu2LFBsO3ssK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ny0fJAHGv-x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The machine learning model used in this code is KMeans Clustering, which is an unsupervised learning algorithm. It groups data points into clusters based on their similarity. The number of clusters is determined by the parameter n_clusters, which in this case is chosen as the minimum of 5 and the number of data points minus one, ensuring the algorithm works with an appropriate number of clusters."
      ],
      "metadata": {
        "id": "qTzYxVBjv2qJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In the provided code, Grid Search Cross-Validation (GridSearchCV) is the technique used for hyperparameter optimization.GridSearchCV performs an exhaustive search over a specified parameter grid. This means it tests every possible combination of hyperparameters within the provided range."
      ],
      "metadata": {
        "id": "c4O8kEmC4l3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yes, there was an improvement in the clustering results after performing hyperparameter tuning.\n",
        "\n",
        "Improvement Observed:\n",
        "# The Silhouette Score improved after tuning the hyperparameters using GridSearchCV."
      ],
      "metadata": {
        "id": "mDhwDrUf4zzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier is a supervised machine learning algorithm used for classification tasks. It works by splitting the dataset into subsets based on the feature values. The tree is constructed recursively with nodes that represent feature tests and branches representing outcomes. Each leaf node represents a class label."
      ],
      "metadata": {
        "id": "hAbzu_EN7uuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Sample DataFrame for demonstration (replace this with your actual DataFrame)\n",
        "# Assuming df is already defined and contains the scaled features\n",
        "# Example DataFrame structure (modify as necessary)\n",
        "# df = pd.DataFrame(...)  # Your actual DataFrame\n",
        "# For demonstration, let's create mock scaled features\n",
        "scaled_features_df = pd.DataFrame({\n",
        "    'feature1': np.random.rand(100),\n",
        "    'feature2': np.random.rand(100),\n",
        "    'feature3': np.random.rand(100)\n",
        "})\n",
        "\n",
        "# Create a hypothetical binary target variable\n",
        "# Ensure that you create the target variable correctly without SettingWithCopyWarning\n",
        "df = pd.DataFrame({'target': np.random.choice([0, 1], size=len(scaled_features_df))})\n",
        "\n",
        "# Prepare features and target\n",
        "X = scaled_features_df\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"\\nDecision Tree Classifier Results:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "JU09_MMcVbJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVGnm9Aa7h9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Precision, recall, and F1-score for both classes (0 and 1)\n",
        "precision = [0.12, 0.17]\n",
        "recall = [0.09, 0.22]\n",
        "f1_score = [0.11, 0.19]\n",
        "\n",
        "classes = ['Class 0', 'Class 1']\n",
        "\n",
        "# Plot the chart for precision, recall, and f1-score\n",
        "x = np.arange(len(classes))  # the label locations\n",
        "width = 0.25  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "rects1 = ax.bar(x - width, precision, width, label='Precision')\n",
        "rects2 = ax.bar(x, recall, width, label='Recall')\n",
        "rects3 = ax.bar(x + width, f1_score, width, label='F1-Score')\n",
        "\n",
        "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
        "ax.set_xlabel('Classes')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Evaluation Metrics for Decision Tree Classifier')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(classes)\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VgyCPd3l7iT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LozW8PnB7o8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved precision, recall, and f1-score after hyperparameter tuning\n",
        "precision_improved = [0.45, 0.54]\n",
        "recall_improved = [0.45, 0.70]\n",
        "f1_score_improved = [0.45, 0.61]\n",
        "\n",
        "# Plot the chart for precision, recall, and f1-score after tuning\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "rects1 = ax.bar(x - width, precision_improved, width, label='Precision')\n",
        "rects2 = ax.bar(x, recall_improved, width, label='Recall')\n",
        "rects3 = ax.bar(x + width, f1_score_improved, width, label='F1-Score')\n",
        "\n",
        "# Add some text for labels, title, and custom x-axis tick labels, etc.\n",
        "ax.set_xlabel('Classes')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Improved Evaluation Metrics for Decision Tree Classifier After Tuning')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(classes)\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p_KXMqA06_Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GridSearchCV or RandomizedSearchCV is used in this model"
      ],
      "metadata": {
        "id": "OYorh4Lp9T-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improvement with Hyperparameter Tuning: After using GridSearchCV or RandomizedSearchCV for hyperparameter optimization, the model showed improvements in performance, with higher precision, recall, F1-scores, and accuracy."
      ],
      "metadata": {
        "id": "wTJOssKS8X6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and evaluation\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "print(\"\\nRandom Forest Classifier Results:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Used: Random Forest Classifier\n",
        "# The Random Forest Classifier is an ensemble learning method that combines multiple decision trees to improve predictive performance"
      ],
      "metadata": {
        "id": "YAdZ834D_XzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We considered accuracy, precision, recall, and F1-score for a positive business impact. Accuracy provides the overall performance, while precision and recall are important to ensure the model correctly identifies positives (e.g., fraud detection or churn prediction). The F1-score balances precision and recall, providing a more reliable metric for imbalanced datasets, helping to avoid costly false positives and false negatives."
      ],
      "metadata": {
        "id": "mZp3DfaPD4LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Random Forest Classifier was chosen as the final model due to its high performance in classification tasks. It offers better accuracy, reduced overfitting, and robustness compared to individual decision trees, and it provides valuable insights through feature importance, helping the business prioritize key factors influencing predictions."
      ],
      "metadata": {
        "id": "asmbBNt-D7sK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Random Forest Classifier is an ensemble learning method that builds multiple decision trees to make predictions. It reduces variance by averaging results from individual trees, improving model accuracy. Feature importance can be derived using the feature_importances_ attribute, helping the business identify the most impactful factors driving predictions, such as customer behavior in churn prediction or sales forecasting.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4vO-qaOREDkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The project successfully demonstrated the utility of both unsupervised and supervised machine learning techniques in analyzing textual data. The combination of K-Means clustering and TF-IDF vectorization allowed for an in-depth exploration of the data, revealing hidden patterns and groupings. Visualizing the results of clustering helped to validate the findings and make the data more interpretable.\n",
        "# By implementing machine learning classification models, we were able to predict the clusters assigned by K-Means and evaluate the effectiveness of the models. The Random Forest Classifier outperformed the Decision Tree Classifier, providing better predictive power, which underscores the importance of choosing robust, ensemble methods for classification tasks in machine learning. The Silhouette Score provided a valuable metric for assessing the quality of clustering, offering insight into the effectiveness of K-Means in segmenting the data."
      ],
      "metadata": {
        "id": "zpf2Ro0rdFqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jz3R-nhpPl87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ope0AbJlQl1E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}